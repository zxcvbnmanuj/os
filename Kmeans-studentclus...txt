K-Means

# Import necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

# Load the dataset from the specified CSV file into a Pandas DataFrame
df = pd.read_csv('C:/Users/HP/Downloads/student_clustering.csv')
df.head()

# Display the shape of the DataFrame (number of rows, number of columns)
df.shape

# Scatter plot of 'cgpa' against 'iq' to visualize the data
plt.scatter(df['cgpa'], df['iq'])

# Create an array 'x' containing all the features in the DataFrame
x = df.iloc[:, :].values

# Initialize an empty list 'wcss' to store the sum of squared distances within clusters
wcss = []

# Loop through a range of potential cluster numbers (1 to 10)
for i in range(1, 11):
    # Create a KMeans object with 'n_clusters=i' clusters and 'n_init=10' initializations
    km = KMeans(n_clusters=i, n_init=10)
    
    # Fit the KMeans model and predict cluster indices for each data point
    km.fit_predict(df)
    
    # Append the inertia (within-cluster sum of squares) to the 'wcss' list
    wcss.append(km.inertia_)

# Plot the Elbow Method graph to find the optimal number of clusters
plt.plot(range(1, 11), wcss)

# Create a KMeans object with the chosen number of clusters (4) and 'n_init=10' initializations
km = KMeans(n_clusters=4, n_init=10)

# Predict cluster indices for each data point
y_means = km.fit_predict(x)

# Display the cluster indices assigned to each data point
y_means

# Display the data points belonging to each cluster with different colors
plt.scatter(x[y_means==0, 0], x[y_means==0, 1], color='blue')
plt.scatter(x[y_means==1, 0], x[y_means==1, 1], color='red')
plt.scatter(x[y_means==2, 0], x[y_means==2, 1], color='green')
plt.scatter(x[y_means==3, 0], x[y_means==3, 1], color='magenta')

# Show the plot
plt.show()
